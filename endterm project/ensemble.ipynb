{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Canadian Hospital Readmittance Challenge \n",
    "\n",
    "This notebook is made as a part of the Machine Learning (AI-511) project. It has been made by the following students - \n",
    "\n",
    "1. Siddharth Kothari (IMT2021019)\n",
    "2. Sankalp Kothari (IMT2021028)\n",
    "3. M Srinivasan (IMT2021058)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "from math import floor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,ConfusionMatrixDisplay\n",
    "import re\n",
    "pd.options.display.max_rows = 4000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model fitting \n",
    "\n",
    "We tried out various combinations of models for the data. Before that, we first carried out a few steps - \n",
    "\n",
    "1. We standard scaled the numerical columns.\n",
    "2. We one-hot encoded all the categorical columns with 3 or more categories. This led to certain columns which were there in the test data but not in the training data (as some categories appeared only in the test data), and some others which were only available in the training data.\n",
    "3. To handle this, we do the following - add all columns from the train data not present in the test data with all values as zero, while the columns in the test data not available in the test data are simply dropped. We then sort the columns based on name to ensure correct ordering in training and testing data.\n",
    "4. We then do a train test split with the test size as 0.2, and proceed to fit the models.\n",
    "5. We also tried introducing polynomial features, but it decreased the performance of the model, so we removed it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_encoded = pd.read_csv(\"./processed_data.csv\")\n",
    "test_encoded = pd.read_csv(\"./test_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = input_encoded.iloc[:, -1]\n",
    "input_encoded = input_encoded.iloc[:, 1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,Y_train,Y_test = train_test_split(input_encoded, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then proceed to try the following ensemble models. We used optuna for hyperparameter tuning, and the best cross validation accuracy scores we got for the models that we trained are as follows - \n",
    "\n",
    "| Model | CV Score |\n",
    "| ----- | -------- |\n",
    "| Random Forest | 0.7114682762492981 |\n",
    "| XGBoost (Gradient Boosting) | 0.7120297585626053 | \n",
    "\n",
    "Based on this we chose xgboost as the best model, and used it to make predictions on the test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def objective(trial):\n",
    "#     criterion = trial.suggest_categorical(\"criterion\", [\"gini\", \"entropy\"])\n",
    "#     max_depth = trial.suggest_int(\"max_depth\", 2, 32, log=True)\n",
    "#     n_estimators = trial.suggest_int(\"n_estimators\", 100,500)\n",
    "#     random_state = trial.suggest_int(\"random_state\",42,42)\n",
    "#     rf = RandomForestClassifier(criterion =criterion,\n",
    "#             max_depth=max_depth, \n",
    "#             n_estimators=n_estimators,\n",
    "#             random_state=random_state\n",
    "#         )\n",
    "#     X_train,X_test,Y_train,Y_test = train_test_split(input_encoded, labels, test_size=0.2, random_state=42)\n",
    "#     rf.fit(X_train,Y_train)\n",
    "#     y_pred = rf.predict(X_test)\n",
    "#     score = accuracy_score(y_pred, Y_test)\n",
    "#     return score\n",
    "\n",
    "\n",
    "# study = optuna.create_study(direction=\"maximize\")\n",
    "# study.optimize(objective, n_trials=15)\n",
    "\n",
    "\n",
    "# def objective2(trial):\n",
    "#     # data, target = sklearn.datasets.load_breast_cancer(return_X_y=True)\n",
    "#     X_train,X_test,Y_train,Y_test = train_test_split(input_encoded, labels, test_size=0.3, random_state=42)\n",
    "#     regex = re.compile(r\"\\[|\\]|<\", re.IGNORECASE)\n",
    "#     dict ={0:1.45,2:1,1:1.4}\n",
    "#     X_train.columns = [regex.sub(\"_\", col) if any(x in str(col) for x in set(('[', ']', '<'))) else col for col in X_train.columns.values]\n",
    "#     X_test.columns = [regex.sub(\"_\", col) if any(x in str(col) for x in set(('[', ']', '<'))) else col for col in X_test.columns.values]\n",
    "\n",
    "#     max_depth = trial.suggest_int(\"max_depth\", 3, 10)\n",
    "#     n_estimators = trial.suggest_int(\"n_estimators\", 200,500)\n",
    "#     learning_rate = trial.suggest_int(\"learning_rate\",0,1)\n",
    "#     # gamma = trial.suggest_int(\"gamma\",0,5)\n",
    "#     reg_lambda = trial.suggest_int(\"reg_lambda\",0,5)\n",
    "#     class_weight = trial.suggest_int(\"class_weight\",0,3)\n",
    "#     rf = xgb.XGBClassifier(\n",
    "#             max_depth=max_depth, \n",
    "#             n_estimators=n_estimators,\n",
    "#             learning_rate=learning_rate,\n",
    "#             reg_lambda=reg_lambda,\n",
    "#             class_weight = class_weight\n",
    "#         )\n",
    "#     rf.fit(X_train,Y_train)\n",
    "#     preds = rf.predict(X_test)\n",
    "#     pred_labels = np.rint(preds)\n",
    "#     accuracy = accuracy_score(Y_test, pred_labels)\n",
    "#     return accuracy\n",
    "\n",
    "# study = optuna.create_study(direction=\"maximize\")\n",
    "# study.optimize(objective2, n_trials=15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr = LogisticRegression(random_state=42, multi_class=\"multinomial\")\n",
    "# lr.fit(X_train,Y_train)\n",
    "\n",
    "# y_pred = lr.predict(X_test)\n",
    "# print(accuracy_score(y_pred, Y_test))\n",
    "\n",
    "# nb = GaussianNB()\n",
    "# nb.fit(X_train,Y_train)\n",
    "\n",
    "# y_pred = nb.predict(X_test)\n",
    "# print(accuracy_score(y_pred, Y_test))\n",
    "\n",
    "# tree = DecisionTreeClassifier(max_depth=20,random_state=42)\n",
    "# tree.fit(X_train,Y_train)\n",
    "\n",
    "# y_pred = tree.predict(X_test)\n",
    "# print(accuracy_score(y_pred, Y_test))\n",
    "\n",
    "# rf = RandomForestClassifier(random_state=42, criterion='entropy', max_depth=30, n_estimators=440)\n",
    "# rf.fit(X_train,Y_train)\n",
    "\n",
    "# y_pred = rf.predict(X_test)\n",
    "# print(accuracy_score(y_pred, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:36:34] WARNING: /croot/xgboost-split_1675457761144/work/src/learner.cc:767: \n",
      "Parameters: { \"class_weight\" } are not used.\n",
      "\n",
      "0.7120297585626053\n"
     ]
    }
   ],
   "source": [
    "regex = re.compile(r\"\\[|\\]|<\", re.IGNORECASE)\n",
    " \n",
    "X_train.columns = [regex.sub(\"_\", col) if any(x in str(col) for x in set(('[', ']', '<'))) else col for col in X_train.columns.values]\n",
    "X_test.columns = [regex.sub(\"_\", col) if any(x in str(col) for x in set(('[', ']', '<'))) else col for col in X_test.columns.values]\n",
    "test_encoded.columns = [regex.sub(\"_\", col) if any(x in str(col) for x in set(('[', ']', '<'))) else col for col in test_encoded.columns.values]\n",
    "\n",
    "\n",
    "xgb = xgb.XGBClassifier(max_depth=3,n_estimators=208,learning_rate=1,reg_lambda=3,class_weight=2)\n",
    "xgb.fit(X_train,Y_train)\n",
    "\n",
    "y_pred = xgb.predict(X_test)\n",
    "print(accuracy_score(y_pred, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_Y = xgb.predict(test_encoded)\n",
    "\n",
    "df_output = pd.read_csv(\"../canadian-hospital-re-admittance-challenge/sample_submission.csv\")\n",
    "df_output[\"readmission_id\"] = test_Y\n",
    "df_output.to_csv(\"submission_xg.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then finally plot the confusion matrix for the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cm = confusion_matrix(Y_test, y_pred, labels=xgb.classes_)\n",
    "# disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=xgb.classes_)\n",
    "# disp.plot()\n",
    "# plt.show()\n",
    "# print(accuracy_score(y_pred, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Confusion_matrix](../images/confusion_matrix.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "1. List of ICD-9 codes : https://en.wikipedia.org/wiki/List_of_ICD-9_codes\n",
    "2. Optuna documentation : https://optuna.readthedocs.io/en/stable/\n",
    "3. Pandas documentation : https://pandas.pydata.org/docs/\n",
    "4. Numpy documentation : https://numpy.org/doc/1.26/user/index.html\n",
    "5. Scikit learn documentation : https://scikit-learn.org/stable/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
